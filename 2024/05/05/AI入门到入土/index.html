
<!DOCTYPE html>
<html lang="zh-CN">

<head>
  <meta charset="UTF-8" />
  <title>AI入门到入土 | 雲流のLowest World</title>
  <meta name="author" content="C1oudfL0w0" />
  <meta name="description" content="" />
  <meta name="keywords" content="" />
  <meta name="viewport" content="width=device-width,initial-scale=1.0,maximum-scale=1.0,user-scalable=0" />
  <link rel="icon" href="/blog/images/croppedImage_cropped.jpg" />
  <!-- cdn挂掉的时候要可以引用本地 -->
<!-- <script src="https://cdn.staticfile.org/vue/3.2.45/vue.global.prod.min.js"></script> -->
<script src="/blog/js/vue.global.prod.min.js"></script>


<link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/6.2.1/css/all.min.css" />


<link rel="stylesheet" href="/blog/css/all.min.css" />
<link rel="stylesheet" href="/blog/css/fonts.min.css" />
<link rel="stylesheet" href="/blog/css/search.css" />
<script> const mixins = {}; </script>


<!--改成了prismjs高亮-->

<script src="/blog/js/lib/prism.js"></script>
<link rel="stylesheet" href="/blog/css/prism.css" rel="stylesheet" />



<script src="/blog/js/lib/preview.js"></script>










<script src="/blog/js/lib/home.js"></script>

<link rel="stylesheet" href="/blog/css/main.css" />
<!-- 引入不蒜子-->

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

<script type="text/javascript" src="/blog/js/lib/codeBlock/codeBlockFuction.js"></script>
<!-- 代码语言 -->
<script type="text/javascript" src="/blog/js/lib/codeBlock/codeLang.js"></script>
<!-- 代码块复制 -->
<!-- 引入clipboard -->

<script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.8/clipboard.min.js"></script>

<script type="text/javascript" src="/blog/js/lib/codeBlock/codeCopy.js"></script>
<!-- 代码块收缩 -->
<script type="text/javascript" src="/blog/js/lib/codeBlock/codeShrink.js"></script> 
<!-- 代码块折行 -->
<style type="text/css">code[class*="language-"], pre[class*="language-"] { white-space: pre !important; }</style>
<meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/blog/atom.xml" title="雲流のLowest World" type="application/atom+xml">
</head>

<body>
  <!--flag{never_g0nna g1ve_you up}-->

  <!-- 页面点击特效 -->
  <script type="text/javascript" src="/blog/js/love-click.js"></script>

  <!-- 浏览器标题 -->
  <script async type="text/javascript" src="/blog/js/FunnyTitle.js"></script>

  <!-- 动态背景 -->
  

  <!--文章目录-->
  
    <div id="toc" class="toc-article">
    <h3><i class="fas fa-stream"></i>  目录</h3>
      <div class="toc-title"></div>
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%89%8D%E8%A8%80"><span class="toc-number">1.</span> <span class="toc-text">前言</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"><span class="toc-number">2.</span> <span class="toc-text">什么是机器学习</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E9%A6%96%E4%B8%AA%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B"><span class="toc-number">3.</span> <span class="toc-text">创建首个机器学习模型</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80"><span class="toc-number">4.</span> <span class="toc-text">大语言模型基础</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%8E-Transformer-%E6%9E%B6%E6%9E%84"><span class="toc-number">4.1.</span> <span class="toc-text">语言模型与 Transformer 架构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%B9%BB%E8%A7%89"><span class="toc-number">4.2.</span> <span class="toc-text">模型幻觉</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9"><span class="toc-number">5.</span> <span class="toc-text">模型选择</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%AC%E5%9C%B0"><span class="toc-number">5.1.</span> <span class="toc-text">本地</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%8F%90%E7%A4%BA%E5%B7%A5%E7%A8%8B"><span class="toc-number">6.</span> <span class="toc-text">提示工程</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E9%87%87%E6%A0%B7%E5%8F%82%E6%95%B0"><span class="toc-number">6.1.</span> <span class="toc-text">模型采样参数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A0%B7%E6%9C%AC%E6%8F%90%E7%A4%BA"><span class="toc-number">6.2.</span> <span class="toc-text">样本提示</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8C%87%E4%BB%A4%E8%B0%83%E4%BC%98"><span class="toc-number">6.3.</span> <span class="toc-text">指令调优</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80%E6%8F%90%E7%A4%BA%E6%8A%80%E5%B7%A7"><span class="toc-number">6.4.</span> <span class="toc-text">基础提示技巧</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%9D%E7%BB%B4%E9%93%BE"><span class="toc-number">6.5.</span> <span class="toc-text">思维链</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Prompt-%E8%AE%BE%E8%AE%A1"><span class="toc-number">6.6.</span> <span class="toc-text">Prompt 设计</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Prompt-%E6%A1%86%E6%9E%B6"><span class="toc-number">6.7.</span> <span class="toc-text">Prompt 框架</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AE%B0%E5%BF%86%E7%B3%BB%E7%BB%9F%EF%BC%88Memory-System%EF%BC%89%E4%B8%8E%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90%EF%BC%88Retrieval-Augmented-Generation-RAG%EF%BC%89"><span class="toc-number">7.</span> <span class="toc-text">记忆系统（Memory System）与检索增强生成（Retrieval-Augmented Generation, RAG）</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%99%BA%E8%83%BD%E4%BD%93%EF%BC%88Agent%EF%BC%89"><span class="toc-number">8.</span> <span class="toc-text">智能体（Agent）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9E%84%E6%88%90%E4%B8%8E%E8%BF%90%E8%A1%8C%E5%8E%9F%E7%90%86"><span class="toc-number">8.1.</span> <span class="toc-text">构成与运行原理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%90%E8%A1%8C%E6%9C%BA%E5%88%B6"><span class="toc-number">8.2.</span> <span class="toc-text">运行机制</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%84%9F%E7%9F%A5%E4%B8%8E%E8%A1%8C%E5%8A%A8"><span class="toc-number">8.3.</span> <span class="toc-text">感知与行动</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0%E7%AC%AC%E4%B8%80%E4%B8%AA%E6%99%BA%E8%83%BD%E4%BD%93"><span class="toc-number">8.4.</span> <span class="toc-text">实现第一个智能体</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%8F%E4%BD%9C%E6%A8%A1%E5%BC%8F"><span class="toc-number">8.5.</span> <span class="toc-text">协作模式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%9C%E4%B8%BA%E5%BC%80%E5%8F%91%E8%80%85%E5%B7%A5%E5%85%B7"><span class="toc-number">8.5.1.</span> <span class="toc-text">作为开发者工具</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%9C%E4%B8%BA%E8%87%AA%E4%B8%BB%E5%8D%8F%E4%BD%9C%E8%80%85"><span class="toc-number">8.5.2.</span> <span class="toc-text">作为自主协作者</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Workflow-%E4%B8%8E-Agent-%E7%9A%84%E5%B7%AE%E5%BC%82"><span class="toc-number">8.5.3.</span> <span class="toc-text">Workflow 与 Agent 的差异</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AD%A3%E5%BC%8F%E6%9E%84%E5%BB%BA%E6%99%BA%E8%83%BD%E4%BD%93%E7%BB%8F%E5%85%B8%E8%8C%83%E5%BC%8F"><span class="toc-number">8.6.</span> <span class="toc-text">正式构建智能体经典范式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%8E%AF%E5%A2%83"><span class="toc-number">8.6.1.</span> <span class="toc-text">环境</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%85%8D%E7%BD%AE%E5%AE%A2%E6%88%B7%E7%AB%AF"><span class="toc-number">8.6.2.</span> <span class="toc-text">配置客户端</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%81%E8%A3%85%E5%9F%BA%E7%A1%80-LLM-%E8%B0%83%E7%94%A8%E5%87%BD%E6%95%B0"><span class="toc-number">8.6.3.</span> <span class="toc-text">封装基础 LLM 调用函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ReAct-%E8%8C%83%E5%BC%8F"><span class="toc-number">8.6.4.</span> <span class="toc-text">ReAct 范式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B7%A5%E5%85%B7%E7%9A%84%E5%AE%9A%E4%B9%89%E4%B8%8E%E5%AE%9E%E7%8E%B0"><span class="toc-number">8.6.5.</span> <span class="toc-text">工具的定义与实现</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#MCP"><span class="toc-number">9.</span> <span class="toc-text">MCP</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#hello-world"><span class="toc-number">9.1.</span> <span class="toc-text">hello world</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#IDA-PRO-MCP"><span class="toc-number">9.2.</span> <span class="toc-text">IDA-PRO-MCP</span></a></li></ol></li></ol>
    </div>
    <script>
    (function () {
        function initTocHighlight() {
            var tocLinks = document.querySelectorAll('#toc .toc-link');
            if (!tocLinks.length) return;

            // 收集所有标题锚点对应的 DOM 元素
            var headings = [];
            tocLinks.forEach(function (link) {
                var href = link.getAttribute('href');
                if (!href) return;
                var id = decodeURIComponent(href.slice(1));
                var el = document.getElementById(id);
                if (el) headings.push({ el: el, link: link });
            });
            if (!headings.length) return;

            var activeLink = null;
            var offset = 80;

            function highlight() {
                var scrollTop = window.pageYOffset || document.documentElement.scrollTop;
                var current = null;

                for (var i = 0; i < headings.length; i++) {
                    if (headings[i].el.getBoundingClientRect().top <= offset + 10) {
                        current = headings[i];
                    }
                }

                // 仅在用户确实滚动过、且到达页面底部时，才强制高亮最后一个
                if (scrollTop > 100 && window.innerHeight + scrollTop >= document.documentElement.scrollHeight - 50) {
                    current = headings[headings.length - 1];
                }

                // 页面顶部且没有标题进入视口时，高亮第一个
                if (scrollTop == 0) {
                    current = headings[0];
                }

                if (current && current.link !== activeLink) {
                    // 清除所有高亮
                    tocLinks.forEach(function (link) {
                        link.classList.remove('toc-active');
                    });

                    // 高亮当前项
                    current.link.classList.add('toc-active');
                    activeLink = current.link;

                    // TOC 容器自动滚动，使当前项可见
                    var tocEl = document.getElementById('toc');
                    var linkRect = activeLink.getBoundingClientRect();
                    var tocRect = tocEl.getBoundingClientRect();
                    if (linkRect.top < tocRect.top || linkRect.bottom > tocRect.bottom) {
                        activeLink.scrollIntoView({ block: 'center', behavior: 'smooth' });
                    }
                } else if (!current && activeLink) {
                    tocLinks.forEach(function (link) {
                        link.classList.remove('toc-active');
                    });
                    activeLink = null;
                }
            }

            var ticking = false;
            window.addEventListener('scroll', function () {
                if (!ticking) {
                    requestAnimationFrame(function () {
                        highlight();
                        ticking = false;
                    });
                    ticking = true;
                }
            });

            // 初始高亮
            highlight();
        }

        // 等待页面完全加载后再初始化（确保文章标题 DOM 已渲染）
        if (document.readyState === 'complete') {
            initTocHighlight();
        } else {
            window.addEventListener('load', initTocHighlight);
        }
    })();
    </script>
  

  <!--返回顶部-->
  <div id="totop" style="position:fixed;bottom:50px;right:60px;font-size: 48px;cursor: pointer;z-index: 10;">
    <a title="返回顶部">↑</a>
  </div>
  <script src="/blog/js/totop.js"></script>

  <div id="layout">
    
    <div id="content-background" ref="contentBackground" data-images="https://pic.imgdb.cn/item/65d61d809f345e8d03b8bc7a.png"></div>
    
    <transition name="fade">
      <div id="loading" v-show="loading">
        <div id="loading-circle">
          <h2>LOADING</h2>
          <p>第一次加载文章图片可能会花费较长时间</p>
          <p>要不挂个梯子试试？（x</p>
          <p>加载过慢请开启缓存&ensp;浏览器默认开启</p>
          <img src="/blog/images/loading.gif" />
        </div>
      </div>
    </transition>
    <transition name="into">
      <div id="main" v-show="!loading">
        <nav id="menu" ref="menu">
    <div class="desktop-menu">
        <a class="title" href="/blog/">
            <span>雲流のLOWEST WORLD</span>
        </a>
        
        <a href="/blog/">
            <i class="fa-solid fa-house fa-fw"></i>
            <span>&ensp;Home</span>
        </a>
        
        <a href="/blog/about">
            <i class="fa-solid fa-id-card fa-fw"></i>
            <span>&ensp;About</span>
        </a>
        
        <a href="/blog/archives">
            <i class="fa-solid fa-box-archive fa-fw"></i>
            <span>&ensp;Archives</span>
        </a>
        
        <a href="/blog/categories">
            <i class="fa-solid fa-bookmark fa-fw"></i>
            <span>&ensp;Categories</span>
        </a>
        
        <a href="/blog/tags">
            <i class="fa-solid fa-tags fa-fw"></i>
            <span>&ensp;Tags</span>
        </a>
        
        <a href="/blog/links">
            <i class="fa-solid fa-link fa-fw"></i>
            <span>&ensp;Links</span>
        </a>
        
        <a href="/blog/search">
            <i class="fa-solid fa-search fa-fw"></i>
            <span>&ensp;Search</span>
        </a>
        
    </div>
    <div id="mobile-menu">
        <div class="curtain" v-show="showMenu" @click="showMenu = !showMenu"></div>
        <div class="title" @click="showMenu = !showMenu">
            <i class="fa-solid fa-bars fa-fw"></i>
            <span>&emsp;雲流のLOWEST WORLD</span>
        </div>
        <transition name="slide">
        <div class="items" v-show="showMenu">
            
            <a href="/blog/">
                <div class="item">
                    <div style="min-width: 20px; max-width: 50px; width: 10%">
                        <i class="fa-solid fa-house fa-fw"></i>
                    </div>
                    <div style="min-width: 100px; max-width: 150%; width: 20%">Home</div>
                </div>
            </a>
            
            <a href="/blog/about">
                <div class="item">
                    <div style="min-width: 20px; max-width: 50px; width: 10%">
                        <i class="fa-solid fa-id-card fa-fw"></i>
                    </div>
                    <div style="min-width: 100px; max-width: 150%; width: 20%">About</div>
                </div>
            </a>
            
            <a href="/blog/archives">
                <div class="item">
                    <div style="min-width: 20px; max-width: 50px; width: 10%">
                        <i class="fa-solid fa-box-archive fa-fw"></i>
                    </div>
                    <div style="min-width: 100px; max-width: 150%; width: 20%">Archives</div>
                </div>
            </a>
            
            <a href="/blog/categories">
                <div class="item">
                    <div style="min-width: 20px; max-width: 50px; width: 10%">
                        <i class="fa-solid fa-bookmark fa-fw"></i>
                    </div>
                    <div style="min-width: 100px; max-width: 150%; width: 20%">Categories</div>
                </div>
            </a>
            
            <a href="/blog/tags">
                <div class="item">
                    <div style="min-width: 20px; max-width: 50px; width: 10%">
                        <i class="fa-solid fa-tags fa-fw"></i>
                    </div>
                    <div style="min-width: 100px; max-width: 150%; width: 20%">Tags</div>
                </div>
            </a>
            
            <a href="/blog/links">
                <div class="item">
                    <div style="min-width: 20px; max-width: 50px; width: 10%">
                        <i class="fa-solid fa-link fa-fw"></i>
                    </div>
                    <div style="min-width: 100px; max-width: 150%; width: 20%">Links</div>
                </div>
            </a>
            
            <a href="/blog/search">
                <div class="item">
                    <div style="min-width: 20px; max-width: 50px; width: 10%">
                        <i class="fa-solid fa-search fa-fw"></i>
                    </div>
                    <div style="min-width: 100px; max-width: 150%; width: 20%">Search</div>
                </div>
            </a>
            
        </div>
        </transition>
    </div>
</nav>

        <div class="article">
    <div>
        <h1>AI入门到入土</h1>
    </div>
    <div class="info">
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2024/5/5
        </span>
        
        
        <!--文章字数统计-->
        
            
<div style="margin-top:10px;">
    <span class="post-time">
      <span class="post-meta-item-icon">
        <i class="fa fa-keyboard-o"></i>
        <span class="post-meta-item-text">  字数统计: </span>
        <!-- 安装插件npm install hexo-wordcount --save -->
        <span class="post-count">10.6k字</span>
      </span>
    </span>

    <span class="post-time">
      &nbsp; | &nbsp;
      <span class="post-meta-item-icon">
        <i class="fa fa-hourglass-half"></i>
        <span class="post-meta-item-text">  阅读时长: </span>
        <span class="post-count">40分</span>
      </span>
    </span>
    
    <!--不蒜子统计访问数-->
    <span id="busuanzi_container_page_pv">
    &nbsp; | &nbsp;
    总文章阅读量：<span id="busuanzi_value_page_pv"></span>次
    </span>
</div>

          
    </div>
    
    <div class="content" v-pre>
        <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>之前做大创的时候有学习需求，没想到时隔一年半载后需要进行系统的学习了，AI 确实改变了许多东西 —— 20251120</p>
<p>参考：</p>
<p><a target="_blank" rel="noopener" href="https://developers.google.com/codelabs/tensorflow-1-helloworld?hl=zh-cn">https://developers.google.com/codelabs/tensorflow-1-helloworld?hl=zh-cn</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/Acmesec/theAIMythbook">https://github.com/Acmesec/theAIMythbook</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/datawhalechina/self-llm">https://github.com/datawhalechina/self-llm</a></p>
<p><a target="_blank" rel="noopener" href="https://datawhalechina.github.io/hello-agents/#/./README">https://datawhalechina.github.io/hello-agents/#/./README</a></p>
<span id="more"></span>

<hr>
<h1 id="什么是机器学习"><a href="#什么是机器学习" class="headerlink" title="什么是机器学习"></a>什么是机器学习</h1><p>考虑构建应用的传统方式，如下图所示：</p>
<p><img src="/blog/2024/05/05/AI%E5%85%A5%E9%97%A8%E5%88%B0%E5%85%A5%E5%9C%9F/image-20240505004015685.png" alt="image-20240505004015685"></p>
<p>我们用编程语言定下”规则“（比如函数），然后再加入数据，这些规则会根据数据来执行操作，然后我们的程序就会提供一个”答案“（比如函数的返回值）</p>
<p>举个不知道恰不恰当的例子：1+2&#x3D;3，其中 1、2 是我们的数据，而<code>+</code>是我们的规则，最后的 3 则是我们的答案</p>
<p>而机器学习检测活动状态的过程非常相似，只是轴线不同：</p>
<p><img src="/blog/2024/05/05/AI%E5%85%A5%E9%97%A8%E5%88%B0%E5%85%A5%E5%9C%9F/image-20240505004242535.png" alt="image-20240505004242535"></p>
<p>我们只需要提供答案和数据，然后机器来推断出答案与数据之间关系的规则</p>
<p>沿用上面的例子：我们只提供了答案 3 和数据 1、2 ，然后让机器来推断从数据到答案之间的运算规则</p>
<p>而在实际的活动检测场景中会有如下表现：</p>
<p><img src="/blog/2024/05/05/AI%E5%85%A5%E9%97%A8%E5%88%B0%E5%85%A5%E5%9C%9F/image-20240505004632756.png" alt="image-20240505004632756"></p>
<p>我们需要收集大量数据，并有效地标记其为“这是步行的样子”或“这是跑步的样子”。然后，计算机可以根据数据推断出决定某一特定活动的不同模式的规则</p>
<p>在传统编程中，代码被编译为通常称为<strong>程序的二进制文件</strong>。在机器学习中，通过数据和标签创建的项称为<strong>模型</strong>。</p>
<p>于是把上面那张机器学习的图表修改一下，将操作的结果视为一个模型，在运行时就是如下所示：</p>
<p><img src="/blog/2024/05/05/AI%E5%85%A5%E9%97%A8%E5%88%B0%E5%85%A5%E5%9C%9F/image-20240505004744062.png" alt="image-20240505004744062"></p>
<p>我们向模型传递一些数据，然后模型会使用从训练中推断的规则进行预测，例如，“该数据似乎表示在步行”或“该数据似乎表示在骑车”。</p>
<hr>
<h1 id="创建首个机器学习模型"><a href="#创建首个机器学习模型" class="headerlink" title="创建首个机器学习模型"></a>创建首个机器学习模型</h1><p>观察下列数字之间的规律：</p>
<table>
<thead>
<tr>
<th>X</th>
<th>-1</th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
</tr>
</thead>
<tbody><tr>
<td>Y</td>
<td>-2</td>
<td>1</td>
<td>4</td>
<td>7</td>
<td>10</td>
<td>13</td>
</tr>
</tbody></table>
<p>我们正常计算的话可以得出 Y&#x3D;3X+1 这样一个方程，即它们之间的关系</p>
<p>而把这个计算的步骤拆开来看的话，有这样的操作：</p>
<ul>
<li>注意到 X 的值从左向右依次增加 1，而 Y 的值相应地依次增加 3</li>
<li>发现 Y 等于 3X 再加或减某一数字</li>
<li>X 为 0 时 Y 为 1</li>
<li>最终得到 Y&#x3D;3X+1</li>
</ul>
<p>这就是利用代码训练模型来识别数据中模式的准确方式</p>
<hr>
<h1 id="大语言模型基础"><a href="#大语言模型基础" class="headerlink" title="大语言模型基础"></a>大语言模型基础</h1><h2 id="语言模型与-Transformer-架构"><a href="#语言模型与-Transformer-架构" class="headerlink" title="语言模型与 Transformer 架构"></a>语言模型与 Transformer 架构</h2><p>概率统计😭</p>
<p>在深度学习兴起之前，统计方法是语言模型的主流。其核心思想是，一个句子出现的概率，等于该句子中每个词出现的条件概率的连乘</p>
<p>马尔科夫假设 (Markov Assumption) 核心思想：我们不必回溯一个词的全部历史，可以近似地认为，一个词的出现概率只与它前面有限的 n−1 个词有关</p>
<hr>
<h2 id="模型幻觉"><a href="#模型幻觉" class="headerlink" title="模型幻觉"></a>模型幻觉</h2><p>模型幻觉（Hallucination）通常指的是大语言模型生成的内容与客观事实、用户输入或上下文信息相矛盾，或者生成了不存在的事实、实体或事件。</p>
<p>幻觉的本质是模型在生成过程中，过度自信地“编造”了信息，而非准确地检索或推理。根据其表现形式，幻觉可以被分为多种类型，例如：</p>
<ul>
<li><strong>事实性幻觉 (Factual Hallucinations)</strong> ： 模型生成与现实世界事实不符的信息。</li>
<li><strong>忠实性幻觉 (Faithfulness Hallucinations)</strong> ： 在文本摘要、翻译等任务中，生成的内容未能忠实地反映源文本的含义。</li>
<li><strong>内在幻觉 (Intrinsic Hallucinations)</strong> ： 模型生成的内容与输入信息直接矛盾。</li>
</ul>
<p>解决幻觉的几种方法：</p>
<ol>
<li><strong>数据层面</strong>： 通过高质量数据清洗、引入事实性知识以及强化学习与人类反馈 (RLHF) 等方式，从源头减少幻觉。</li>
<li><strong>模型层面</strong>： 探索新的模型架构，或让模型能够表达其对生成内容的不确定性。</li>
<li><strong>推理与生成层面</strong>：<ol>
<li><strong>检索增强生成 (Retrieval-Augmented Generation, RAG)</strong> ： 这是目前缓解幻觉的有效方法之一。RAG 系统通过在生成之前从外部知识库（如文档数据库、网页）中检索相关信息，然后将检索到的信息作为上下文，引导模型生成基于事实的回答。</li>
<li><strong>多步推理与验证</strong>： 引导模型进行多步推理，并在每一步进行自我检查或外部验证。</li>
<li><strong>引入外部工具</strong>： 允许模型调用外部工具（如搜索引擎、计算器、代码解释器）来获取实时信息或进行精确计算。</li>
</ol>
</li>
</ol>
<hr>
<h1 id="模型选择"><a href="#模型选择" class="headerlink" title="模型选择"></a>模型选择</h1><ul>
<li><strong>性能与能力</strong>：这是最核心的考量。不同的模型擅长的任务不同，有的长于逻辑推理和代码生成，有的则在创意写作或多语言翻译上更胜一筹。您可以参考一些公开的基准测试排行榜（如 LMSys Chatbot Arena Leaderboard，大模型竞技场）来评估模型的综合能力。</li>
<li><strong>成本</strong>：对于闭源模型，成本主要体现在 API 调用费用，通常按 Token 数量计费。对于开源模型，成本则体现在本地部署所需的硬件（GPU、内存）和运维上。需要根据应用的预期使用量和预算做出选择。</li>
<li><strong>速度（延迟）</strong>：对于需要实时交互的智能体（如客服、游戏 NPC），模型的响应速度至关重要。一些轻量级或经过优化的模型（如 GPT-3.5 Turbo, Claude 3.5 Sonnet）在延迟上表现更优。</li>
<li><strong>上下文窗口</strong>：模型能一次性处理的 Token 数量上限。对于需要理解长文档、分析代码库或维持长期对话记忆的智能体，选择一个拥有较大上下文窗口（如 128K Token 或更高）的模型是必要的。</li>
<li><strong>部署方式</strong>：使用 API 的方式最简单便捷，但数据需要发送给第三方，且受限于服务商的条款。本地部署则能确保数据隐私和最高程度的自主可控，但对技术和硬件要求更高。</li>
<li><strong>生态与工具链</strong>：一个模型的流行程度也决定了其周边生态的成熟度。主流模型通常拥有更丰富的社区支持、教程、预训练模型、微调工具和兼容的开发框架（如 LangChain, LlamaIndex, Hugging Face Transformers），这能极大地加速开发进程，降低开发难度。选择一个拥有活跃社区和完善工具链的模型，可以在遇到问题时更容易找到解决方案和资源。</li>
<li><strong>可微调性与定制化</strong>：对于需要处理特定领域数据或执行特定任务的智能体，模型的微调能力至关重要。一些模型提供了便捷的微调接口和工具，允许开发者使用自己的数据集对模型进行定制化训练，从而显著提升模型在特定场景下的性能和准确性。开源模型在这方面通常提供更大的灵活性。</li>
<li><strong>安全性与伦理</strong>：随着大语言模型的广泛应用，其潜在的安全风险和伦理问题也日益凸显。选择模型时，需要考虑其在偏见、毒性、幻觉等方面的表现，以及服务商或开源社区在模型安全和负责任AI方面的投入。对于面向公众或涉及敏感信息的应用，模型的安全性和伦理合规性是不可忽视的考量。</li>
</ul>
<h2 id="本地"><a href="#本地" class="headerlink" title="本地"></a>本地</h2><p>考虑到本人的机器是 48g 的内存，选择 30b 左右的模型比较合适，这样还能剩出内存给 IDE、虚拟机、docker、浏览器等</p>
<p>大模型竞技场： <a target="_blank" rel="noopener" href="https://lmarena.ai/zh/leaderboard/text">https://lmarena.ai/zh/leaderboard/text</a></p>
<p>以前百的开源模型为例：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/zai-org/GLM-4.7" title="glm-4.7">glm-4.7</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/moonshotai/Kimi-K2-Thinking" title="kimi-k2-thinking-turbo">kimi-k2-thinking-turbo</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.z.ai/guides/llm/glm-4.6" title="glm-4.6">glm-4.6</a></li>
<li><a target="_blank" rel="noopener" href="https://api-docs.deepseek.com/news/news250929" title="deepseek-v3.2-exp">deepseek-v3.2-exp</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507" title="qwen3-235b-a22b-instruct-2507">qwen3-235b-a22b-instruct-2507</a></li>
<li><a target="_blank" rel="noopener" href="https://api-docs.deepseek.com/news/news250929" title="deepseek-v3.2-exp-thinking">deepseek-v3.2-exp-thinking</a></li>
<li><a target="_blank" rel="noopener" href="https://api-docs.deepseek.com/news/news250929" title="deepseek-v3.2">deepseek-v3.2</a></li>
<li><a target="_blank" rel="noopener" href="https://api-docs.deepseek.com/news/news250929" title="deepseek-v3.2-thinking">deepseek-v3.2-thinking</a></li>
<li><a target="_blank" rel="noopener" href="https://api-docs.deepseek.com/news/news250528" title="deepseek-r1-0528">deepseek-r1-0528</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/moonshotai/Kimi-K2-Instruct-0905" title="kimi-k2-0905-preview">kimi-k2-0905-preview</a></li>
<li><a target="_blank" rel="noopener" href="https://api-docs.deepseek.com/news/news250821" title="deepseek-v3.1">deepseek-v3.1</a></li>
<li><a target="_blank" rel="noopener" href="https://moonshotai.github.io/Kimi-K2/" title="kimi-k2-0711-preview">kimi-k2-0711-preview</a></li>
<li><a target="_blank" rel="noopener" href="https://api-docs.deepseek.com/news/news250821" title="deepseek-v3.1-thinking">deepseek-v3.1-thinking</a></li>
<li><a target="_blank" rel="noopener" href="https://api-docs.deepseek.com/news/news250922" title="deepseek-v3.1-terminus">deepseek-v3.1-terminus</a></li>
<li><a target="_blank" rel="noopener" href="https://qwen.ai/blog?id=99f0335c4ad9ff6153e517418d48535ab6d8afef&from=research.latest-advancements-list" title="qwen3-vl-235b-a22b-instruct">qwen3-vl-235b-a22b-instruct</a></li>
<li><a target="_blank" rel="noopener" href="https://api-docs.deepseek.com/news/news250922" title="deepseek-v3.1-terminus-thinking">deepseek-v3.1-terminus-thinking</a></li>
<li><a target="_blank" rel="noopener" href="https://mistral.ai/news/mistral-3" title="mistral-large-3">mistral-large-3</a></li>
<li><a target="_blank" rel="noopener" href="https://z.ai/blog/glm-4.5" title="glm-4.5">glm-4.5</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct" title="qwen3-next-80b-a3b-instruct">qwen3-next-80b-a3b-instruct</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/meituan-longcat/LongCat-Flash-Chat" title="longcat-flash-chat">longcat-flash-chat</a></li>
<li><a target="_blank" rel="noopener" href="https://qwenlm.github.io/blog/qwen3/" title="qwen3-235b-a22b-no-thinking">qwen3-235b-a22b-no-thinking</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507" title="qwen3-235b-a22b-thinking-2507">qwen3-235b-a22b-thinking-2507</a></li>
<li><a target="_blank" rel="noopener" href="https://mimo.xiaomi.com/blog/mimo-v2-flash" title="mimo-v2-flash (non-thinking)">mimo-v2-flash (non-thinking)</a></li>
<li><a target="_blank" rel="noopener" href="https://api-docs.deepseek.com/news/news250120" title="deepseek-r1">deepseek-r1</a></li>
<li><a target="_blank" rel="noopener" href="https://qwen.ai/blog?id=99f0335c4ad9ff6153e517418d48535ab6d8afef&from=research.latest-advancements-list" title="qwen3-vl-235b-a22b-thinking">qwen3-vl-235b-a22b-thinking</a></li>
<li><a target="_blank" rel="noopener" href="https://api-docs.deepseek.com/news/news250325" title="deepseek-v3-0324">deepseek-v3-0324</a></li>
<li><a target="_blank" rel="noopener" href="https://qwenlm.github.io/blog/qwen3-coder/" title="qwen3-coder-480b-a35b-instruct">qwen3-coder-480b-a35b-instruct</a></li>
<li><a target="_blank" rel="noopener" href="https://www.minimax.io/news/minimaxm1" title="minimax-m2.1-preview">minimax-m2.1-preview</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507" title="qwen3-30b-a3b-instruct-2507">qwen3-30b-a3b-instruct-2507</a></li>
<li><a target="_blank" rel="noopener" href="https://qwenlm.github.io/blog/qwen3/" title="qwen3-235b-a22b">qwen3-235b-a22b</a></li>
<li><a target="_blank" rel="noopener" href="https://z.ai/blog/glm-4.5" title="glm-4.5-air">glm-4.5-air</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Thinking" title="qwen3-next-80b-a3b-thinking">qwen3-next-80b-a3b-thinking</a></li>
<li><a target="_blank" rel="noopener" href="https://www.minimax.io/news/minimaxm1" title="minimax-m1">minimax-m1</a></li>
<li><a target="_blank" rel="noopener" href="http://aistudio.google.com/app/prompts/new_chat?model=gemma-3-27b-it" title="gemma-3-27b-it">gemma-3-27b-it</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/deepseek-ai/DeepSeek-V3" title="deepseek-v3">deepseek-v3</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506" title="mistral-small-2506">mistral-small-2506</a></li>
<li><a target="_blank" rel="noopener" href="https://www.primeintellect.ai/blog/intellect-3" title="intellect-3">intellect-3</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/zai-org/GLM-4.5V" title="glm-4.5v">glm-4.5v</a></li>
<li><a target="_blank" rel="noopener" href="https://openai.com/index/introducing-gpt-oss/" title="gpt-oss-120b">gpt-oss-120b</a></li>
</ul>
<p>那么选择就很稀少了，主要就是 qwen3 和 gemma3 了，考虑到社区生态，这里选择 qwen3-30b-a3b-instruct-2507 作为本地模型，最大上下文长度高达 262144</p>
<hr>
<h1 id="提示工程"><a href="#提示工程" class="headerlink" title="提示工程"></a>提示工程</h1><p>如果我们把大语言模型比作一个能力极强的“大脑”，那么<strong>提示 (Prompt)</strong> 就是我们与这个“大脑”沟通的语言。提示工程，就是研究如何设计出精准的提示，从而引导模型产生我们期望输出的回复。对于构建智能体而言，一个精心设计的提示能让智能体之间协作分工变得高效。</p>
<h2 id="模型采样参数"><a href="#模型采样参数" class="headerlink" title="模型采样参数"></a>模型采样参数</h2><p>本质是通过调整模型对 “概率分布” 的采样策略，让输出匹配具体场景需求，配置合适的参数可以提升 Agent 在特定场景的性能。</p>
<p><code>Temperature</code>：温度是控制模型输出 “随机性” 与 “确定性” 的关键参数。</p>
<ul>
<li>低温度（0 ⩽ Temperature &lt; 0.3）时输出更 “精准、确定”。适用场景： 事实性任务：如问答、数据计算、代码生成；严谨性场景：法律条文解读、技术文档撰写、学术概念解释等场景。</li>
<li>中温度（0.3 ⩽ Temperature &lt; 0.7）：输出 “平衡、自然”。适用场景： 日常对话：如客服交互、聊天机器人；常规创作：如邮件撰写、产品文案、简单故事创作。</li>
<li>高温度（0.7 ⩽ Temperature &lt; 2）：输出 “创新、发散”。适用场景： 创意性任务：如诗歌创作、科幻故事构思、广告 slogan brainstorm、艺术灵感启发；发散性思考。</li>
</ul>
<p><code>Top-k</code>：原理是将所有 token 按概率从高到低排序，取排名前 k 个的 token 组成 “候选集”，随后对筛选出的 k 个 token 的概率进行 “归一化”</p>
<p><code>Top-p</code> ：其原理是将所有 token 按概率从高到低排序，从排序后的第一个 token 开始，逐步累加概率，直到累积和首次达到或超过阈值 p</p>
<p>在文本生成中，当同时设置 Top-p、Top-k 和温度系数时，这些参数会按照分层过滤的方式协同工作，其优先级顺序为：温度调整→Top-k→Top-p。</p>
<p>温度调整整体分布的陡峭程度，Top-k 会先保留概率最高的 k 个候选，然后 Top-p 会从 Top-k 的结果中选取累积概率≥p 的最小集合作为最终的候选集。不过，通常 Top-k 和 Top-p 二选一即可，若同时设置，实际候选集为两者的交集。</p>
<p>需要注意的是，如果将温度设置为 0，则 Top-k 和 Top-p 将变得无关紧要，因为最有可能的 Token 将成为下一个预测的 Token；如果将 Top-k 设置为 1，温度和 Top-p 也将变得无关紧要，因为只有一个 Token 通过 Top-k 标准，它将是下一个预测的 Token。</p>
<h2 id="样本提示"><a href="#样本提示" class="headerlink" title="样本提示"></a>样本提示</h2><p>根据我们给模型提供示例（Exemplar）的数量，提示可以分为三种类型。</p>
<ul>
<li>**零样本提示(Zero-shot Prompting)**：我们不给模型任何示例，直接让它根据指令完成任务。这得益于模型在海量数据上预训练后获得的强大泛化能力。</li>
</ul>
<pre class="line-numbers language-txt" data-language="txt"><code class="language-txt">文本:Datawhale的AI Agent课程非常棒！
情感:正面<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<ul>
<li>**单样本提示 (One-shot Prompting)**：我们给模型提供一个完整的示例，向它展示任务的格式和期望的输出风格。</li>
</ul>
<pre class="line-numbers language-txt" data-language="txt"><code class="language-txt">文本:这家餐厅的服务太慢了。
情感:负面

文本:Datawhale的AI Agent课程非常棒！
情感:<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<ul>
<li><strong>少样本提示 (Few-shot Prompting)</strong> ：我们提供多个示例，这能让模型更准确地理解任务的细节、边界和细微差别，从而获得更好的性能。</li>
</ul>
<pre class="line-numbers language-txt" data-language="txt"><code class="language-txt">文本:这家餐厅的服务太慢了。
情感:负面

文本:这部电影的情节很平淡。
情感:中性

文本:Datawhale的AI Agent课程非常棒！
情感:<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h2 id="指令调优"><a href="#指令调优" class="headerlink" title="指令调优"></a>指令调优</h2><p>早期的 GPT 模型（如 GPT-3）主要是“文本补全”模型，它们擅长根据前面的文本续写，但不一定能很好地理解并执行人类的指令。<del>（事实上，这个博客主题的早期魔改也是本人在 GPT-3 时期的产物）</del></p>
<p><strong>指令调优 (Instruction Tuning)</strong> 是一种微调技术，它使用大量“指令-回答”格式的数据对预训练模型进行进一步的训练。经过指令调优后，模型能更好地理解并遵循用户的指令。我们今天日常工作学习中使用的所有模型（如 <code>ChatGPT</code>, <code>DeepSeek</code>, <code>Qwen</code>）都是其模型家族中经过指令调优过的模型。</p>
<p>对“文本补全”模型的提示(你需要用少样本提示“教会”模型做什么)：</p>
<pre class="line-numbers language-none"><code class="language-none">这是一段将英文翻译成中文的程序。
英文:Hello
中文:你好
英文:How are you?
中文:<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>对“指令调优”模型的提示(你可以直接下达指令)：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">请将下面的英文翻译成中文<span class="token punctuation">:</span>
How are you?<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>


<h2 id="基础提示技巧"><a href="#基础提示技巧" class="headerlink" title="基础提示技巧"></a>基础提示技巧</h2><p><strong>角色扮演 (Role-playing)</strong> 通过赋予模型一个特定的角色，我们可以引导它的回答风格、语气和知识范围，使其输出更符合特定场景的需求。</p>
<pre class="line-numbers language-txt" data-language="txt"><code class="language-txt"># 案例
你现在是一位资深的Python编程专家。请解释一下Python中的GIL（全局解释器锁）是什么，要让一个初学者也能听懂。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p><strong>上下文示例 (In-context Example)</strong> 这与少样本提示的思想一致，通过在提示中提供清晰的输入输出示例，来“教会”模型如何处理我们的请求，尤其是在处理复杂格式或特定风格的任务时非常有效。</p>
<pre class="line-numbers language-txt" data-language="txt"><code class="language-txt"># 案例
我需要你从产品评论中提取产品名称和用户情感。请严格按照下面的JSON格式输出。

评论:这款“星尘”笔记本电脑的屏幕显示效果惊人，但我不太喜欢它的键盘手感。
输出:&#123;"product_name": "星尘笔记本电脑", "sentiment": "混合"&#125;

评论:我刚买的“声动”耳机音质很棒，续航也超出了我的预期！
输出:<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>


<h2 id="思维链"><a href="#思维链" class="headerlink" title="思维链"></a>思维链</h2><p>对于需要逻辑推理、计算或多步骤思考的复杂问题，直接让模型给出答案往往容易出错。<strong>思维链 (Chain-of-Thought, CoT)</strong> 是一种强大的提示技巧，它通过引导模型“一步一步地思考”，提升了模型在复杂任务上的推理能力。</p>
<p>实现 CoT 的关键，是在提示中加入一句简单的引导语，如“请逐步思考”或“Let’s think step by step”。</p>
<pre class="line-numbers language-txt" data-language="txt"><code class="language-txt"># 思维链提示
一个篮球队在一个赛季的80场比赛中赢了60%。在接下来的赛季中，他们打了15场比赛，赢了12场。两个赛季的总胜率是多少？
请一步一步地思考并解答。

>>>
（模型可能会输出）
好的，我们来一步步计算。
第一步:计算第一个赛季赢得的比赛数。
80场 * 60% = 48场。
第二步:计算两个赛季的总比赛数和总胜利数。
总比赛数 = 80 + 15 = 95场。
总胜利数 = 48 + 12 = 60场。
第三步:计算总胜率。
总胜率 = (总胜利数 / 总比赛数) * 100% = (60 / 95) * 100% ≈ 63.16%。
所以，两个赛季的总胜率约为63.16%。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>


<p>通过显式地展示其推理过程，模型不仅更容易得出正确的答案，也让它的回答变得更可信、更易于我们检查和纠正。</p>
<h2 id="Prompt-设计"><a href="#Prompt-设计" class="headerlink" title="Prompt 设计"></a>Prompt 设计</h2><p>Prompt 是指你向 AI 输入的内容，它直接指示AI该做什么任务或生成什么样的输出，简而言之， Prompt 就是你与 AI 之间的“对话内容”，可以是问题、指令、描述或者任务要求，目的是引导AI进行特定的推理，生成或操作，从而得到预期的结果</p>
<p>Prompt 设计的质量直接决定 AI 输出的质量，一个好的 Prompt 能帮助 AI 快速理解任务要求，生成精准的结果；而一个模糊、模棱两可的 Prompt 会导致 AI 给出无关或错误的答案</p>
<p>例：</p>
<pre class="line-numbers language-txt" data-language="txt"><code class="language-txt">模糊 Prompt：分析一个漏洞并给出解决方案
AI 输出：
"漏洞可能是系统漏洞、配置漏洞或逻辑漏洞，建议排查并修复问题"
结果：输出笼统，缺乏具体性，难以直接应用

精准 Prompt："针对一个电商平台的购物车功能，请分析可能存在的业务逻辑漏洞，并说明漏洞影响及修复方案"
AI 输出：
"可能的业务逻辑漏洞是购物车商品价格篡改攻击者通过抓包工具修改请求中的商品价格，支付时支付低于实际价格的金额影响包括财务损失和用户信任下降修复方案：在服务器端重新验证商品价格，确保最终支付金额以数据库为准"
结果：输出具体、实用，直接解决问题<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>


<h2 id="Prompt-框架"><a href="#Prompt-框架" class="headerlink" title="Prompt 框架"></a>Prompt 框架</h2><table>
<thead>
<tr>
<th>框架</th>
<th>适用场景</th>
<th>核心要素</th>
<th>示例任务</th>
</tr>
</thead>
<tbody><tr>
<td>Google 提示词</td>
<td>复杂漏洞分析、攻防模拟</td>
<td>逐步思考、任务分解、限定范围</td>
<td>Web应用漏洞分析</td>
</tr>
<tr>
<td>LangGPT</td>
<td>多变量、多阶段任务</td>
<td>模板、变量、角色</td>
<td>外部渗透测试</td>
</tr>
<tr>
<td>TAG</td>
<td>简单明确任务</td>
<td>任务、行动、目标</td>
<td>日志分析</td>
</tr>
<tr>
<td>COAST</td>
<td>信息全面复杂任务</td>
<td>背景、目标、行动、支持、技术</td>
<td>勒索病毒响应</td>
</tr>
<tr>
<td>APE</td>
<td>结果导向任务</td>
<td>行动、目的、期望</td>
<td>防火墙规则优化</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<hr>
<h1 id="记忆系统（Memory-System）与检索增强生成（Retrieval-Augmented-Generation-RAG）"><a href="#记忆系统（Memory-System）与检索增强生成（Retrieval-Augmented-Generation-RAG）" class="headerlink" title="记忆系统（Memory System）与检索增强生成（Retrieval-Augmented Generation, RAG）"></a>记忆系统（Memory System）与检索增强生成（Retrieval-Augmented Generation, RAG）</h1><p>根据认知心理学的研究，人类记忆可以分为以下几个层次：</p>
<ol>
<li><strong>感觉记忆（Sensory Memory）</strong>：持续时间极短（0.5-3秒），容量巨大，负责暂时保存感官接收到的所有信息</li>
<li><strong>工作记忆（Working Memory）</strong>：持续时间短（15-30秒），容量有限（7±2个项目），负责当前任务的信息处理</li>
<li><strong>长期记忆（Long-term Memory）</strong>：持续时间长（可达终生），容量几乎无限，进一步分为：<ul>
<li><strong>程序性记忆</strong>：技能和习惯（如骑自行车）</li>
<li><strong>陈述性记忆</strong>：可以用语言表达的知识，又分为：<ul>
<li><strong>语义记忆</strong>：一般知识和概念（如”巴黎是法国首都”）</li>
<li><strong>情景记忆</strong>：个人经历和事件（如”昨天的会议内容”）</li>
</ul>
</li>
</ul>
</li>
</ol>
<p>对于基于LLM的智能体而言，通常面临两个根本性局限：<strong>对话状态的遗忘</strong>和<strong>内置知识的局限</strong>。</p>
<hr>
<h1 id="智能体（Agent）"><a href="#智能体（Agent）" class="headerlink" title="智能体（Agent）"></a>智能体（Agent）</h1><p>智能体被定义为任何能够通过 <strong>传感器（Sensors）</strong> 感知其所处 <strong>环境（Environment）</strong> ，并<strong>自主</strong>地通过 <strong>执行器（Actuators）</strong> 采取 <strong>行动（Action）</strong> 以达成特定目标的实体。</p>
<h2 id="构成与运行原理"><a href="#构成与运行原理" class="headerlink" title="构成与运行原理"></a>构成与运行原理</h2><p>要理解智能体的运作，我们必须先理解它所处的<strong>任务环境</strong>。在人工智能领域，通常使用 <strong>PEAS 模型</strong> 来精确描述一个任务环境：<strong>性能度量(Performance)、环境(Environment)、执行器(Actuators)和传感器(Sensors)</strong></p>
<p>以智能旅行助手为例，其 PEAS 为：</p>
<p><img src="/blog/2024/05/05/AI%E5%85%A5%E9%97%A8%E5%88%B0%E5%85%A5%E5%9C%9F/IMG-20260115174145166.png"></p>
<ol>
<li>环境往往是<strong>部分可观察的</strong>。例如，旅行助手在查询航班时，无法一次性获取所有航空公司的全部实时座位信息。它只能通过调用航班预订 API，看到该 API 返回的部分数据，这就要求智能体必须具备记忆（记住已查询过的航线）和探索（尝试不同的查询日期）的能力。</li>
<li>行动的结果也并非总是确定的。根据结果的可预测性，环境可分为<strong>确定性</strong>和<strong>随机性</strong>。旅行助手的任务环境就是典型的随机性环境。当它搜索票价时，两次相邻的调用返回的机票价格和余票数量都可能不同，这就要求智能体必须具备处理不确定性、监控变化并及时决策的能力。</li>
<li>环境中还可能存在其他行动者，从而形成<strong>多智能体(Multi-agent)</strong> 环境。对于旅行助手而言，其他用户的预订行为、其他自动化脚本，甚至航司的动态调价系统，都是环境中的其他“智能体”。它们的行动（例如，订走最后一张特价票）会直接改变旅行助手所处环境的状态，这对智能体的快速响应和策略选择提出了更高要求。</li>
<li>几乎所有任务都发生在<strong>序贯</strong>且<strong>动态</strong>的环境中。“序贯”意味着当前动作会影响未来；而“动态”则意味着环境自身可能在智能体决策时发生变化。这就要求智能体的“感知-思考-行动-观察”循环必须能够快速、灵活地适应持续变化的世界。</li>
</ol>
<hr>
<h2 id="运行机制"><a href="#运行机制" class="headerlink" title="运行机制"></a>运行机制</h2><p>智能体并非一次性完成任务，而是通过一个持续的循环与环境进行交互，这个核心机制被称为 <strong>智能体循环 (Agent Loop)</strong></p>
<p><img src="/blog/2024/05/05/AI%E5%85%A5%E9%97%A8%E5%88%B0%E5%85%A5%E5%9C%9F/IMG-20260115174910556.png"></p>
<ol>
<li>**感知 (Perception)<strong>：这是循环的起点。智能体通过其传感器（例如，API 的监听端口、用户输入接口）接收来自环境的输入信息。这些信息，即</strong>观察 (Observation)**，既可以是用户的初始指令，也可以是上一步行动所导致的环境状态变化反馈。</li>
<li>**思考 (Thought)**：接收到观察信息后，智能体进入其核心决策阶段。对于 LLM 智能体而言，这通常是由大语言模型驱动的内部推理过程。如图所示，“思考”阶段可进一步细分为两个关键环节：<ul>
<li>**规划 (Planning)**：智能体基于当前的观察和其内部记忆，更新对任务和环境的理解，并制定或调整一个行动计划。这可能涉及将复杂目标分解为一系列更具体的子任务。</li>
<li>**工具选择 (Tool Selection)**：根据当前计划，智能体从其可用的工具库中，选择最适合执行下一步骤的工具，并确定调用该工具所需的具体参数。</li>
</ul>
</li>
<li>**行动 (Action)**：决策完成后，智能体通过其执行器（Actuators）执行具体的行动。这通常表现为调用一个选定的工具（如代码解释器、搜索引擎 API），从而对环境施加影响，意图改变环境的状态。</li>
</ol>
<p>行动并非循环的终点。智能体的行动会引起<strong>环境 (Environment)</strong> 的<strong>状态变化 (State Change)<strong>，环境随即会产生一个新的</strong>观察 (Observation)</strong> 作为结果反馈。这个新的观察又会在下一轮循环中被智能体的感知系统捕获，形成一个持续的“感知-思考-行动-观察”的闭环。智能体正是通过不断重复这一循环，逐步推进任务，从初始状态向目标状态演进。</p>
<h2 id="感知与行动"><a href="#感知与行动" class="headerlink" title="感知与行动"></a>感知与行动</h2><p>在工程实践中，为了让 LLM 能够有效驱动这个循环，我们需要一套明确的<strong>交互协议 (Interaction Protocol)</strong> 来规范其与环境之间的信息交换。</p>
<p>在许多现代智能体框架中，这一协议体现在对智能体每一次输出的结构化定义上。智能体的输出不再是单一的自然语言回复，而是一段遵循特定格式的文本，其中明确地展示了其内部的推理过程与最终决策。</p>
<p>这个结构通常包含两个核心部分：</p>
<ul>
<li>**Thought (思考)**：这是智能体内部决策的“快照”。它以自然语言形式阐述了智能体如何分析当前情境、回顾上一步的观察结果、进行自我反思与问题分解，并最终规划出下一步的具体行动。</li>
<li>**Action (行动)**：这是智能体基于思考后，决定对环境施加的具体操作，通常以函数调用的形式表示。</li>
</ul>
<p>例如，一个正在规划旅行的智能体可能会生成如下格式化的输出：</p>
<pre class="line-numbers language-none"><code class="language-none">Thought: 用户想知道北京的天气。我需要调用天气查询工具。
Action: get_weather(&quot;北京&quot;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p>这里的 <code>Action</code> 字段构成了对外部世界的指令。一个外部的<strong>解析器 (Parser)</strong> 会捕捉到这个指令，并调用相应的 <code>get_weather</code> 函数。</p>
<p>行动执行后，环境会返回一个结果。例如，<code>get_weather</code> 函数可能返回一个包含详细天气数据的 JSON 对象。然而，原始的机器可读数据（如 JSON）通常包含 LLM 无需关注的冗余信息，且格式不符合其自然语言处理的习惯。</p>
<p>因此，感知系统的一个重要职责就是扮演传感器的角色：将这个原始输出处理并封装成一段简洁、清晰的自然语言文本，即观察。</p>
<pre class="line-numbers language-none"><code class="language-none">Observation: 北京当前天气为晴，气温25摄氏度，微风。<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>这段 <code>Observation</code> 文本会被反馈给智能体，作为下一轮循环的主要输入信息，供其进行新一轮的 <code>Thought</code> 和 <code>Action</code>。</p>
<p>于是这三者构成了 <code>Thought-Action-Observation</code> 的交互范式</p>
<hr>
<h2 id="实现第一个智能体"><a href="#实现第一个智能体" class="headerlink" title="实现第一个智能体"></a>实现第一个智能体</h2><p>这里尝试构建一个基本的 CTF Agent，能够访问网页与一些基本渗透工具的使用</p>
<p>使用 uv 构建一个新项目并安装依赖</p>
<pre class="line-numbers language-zsh" data-language="zsh"><code class="language-zsh">uv init
uv add requests openai<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p>指令模板：</p>
<p>驱动真实 LLM 的关键在于<strong>提示工程（Prompt Engineering）</strong>。我们需要设计一个“指令模板”，告诉 LLM 它应该扮演什么角色、拥有哪些工具、以及如何格式化它的思考和行动。这是我们智能体的“说明书”，它将作为 <code>system_prompt</code> 传递给 LLM。</p>
<p>先准备一个客户端调用 OpenAI 接口</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> openai <span class="token keyword">import</span> OpenAI

<span class="token keyword">class</span> <span class="token class-name">OpenAICompatibleClient</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    一个用于调用任何兼容OpenAI接口的LLM服务的客户端。
    """</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> model<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">,</span> api_key<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">,</span> base_url<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>model <span class="token operator">=</span> model
        self<span class="token punctuation">.</span>client <span class="token operator">=</span> OpenAI<span class="token punctuation">(</span>api_key<span class="token operator">=</span>api_key<span class="token punctuation">,</span> base_url<span class="token operator">=</span>base_url<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">generate</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> prompt<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">,</span> system_prompt<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token builtin">str</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""调用LLM API来生成回应。"""</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"正在调用大语言模型..."</span><span class="token punctuation">)</span>
        <span class="token keyword">try</span><span class="token punctuation">:</span>
            messages <span class="token operator">=</span> <span class="token punctuation">[</span>
                <span class="token punctuation">&#123;</span><span class="token string">'role'</span><span class="token punctuation">:</span> <span class="token string">'system'</span><span class="token punctuation">,</span> <span class="token string">'content'</span><span class="token punctuation">:</span> system_prompt<span class="token punctuation">&#125;</span><span class="token punctuation">,</span>
                <span class="token punctuation">&#123;</span><span class="token string">'role'</span><span class="token punctuation">:</span> <span class="token string">'user'</span><span class="token punctuation">,</span> <span class="token string">'content'</span><span class="token punctuation">:</span> prompt<span class="token punctuation">&#125;</span>
            <span class="token punctuation">]</span>
            response <span class="token operator">=</span> self<span class="token punctuation">.</span>client<span class="token punctuation">.</span>chat<span class="token punctuation">.</span>completions<span class="token punctuation">.</span>create<span class="token punctuation">(</span>
                model<span class="token operator">=</span>self<span class="token punctuation">.</span>model<span class="token punctuation">,</span>
                messages<span class="token operator">=</span>messages<span class="token punctuation">,</span>
                stream<span class="token operator">=</span><span class="token boolean">False</span>
            <span class="token punctuation">)</span>
            answer <span class="token operator">=</span> response<span class="token punctuation">.</span>choices<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>message<span class="token punctuation">.</span>content
            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"大语言模型响应成功。"</span><span class="token punctuation">)</span>
            <span class="token keyword">return</span> answer
        <span class="token keyword">except</span> Exception <span class="token keyword">as</span> e<span class="token punctuation">:</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"调用LLM API时发生错误: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>e<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
            <span class="token keyword">return</span> <span class="token string">"错误:调用语言模型服务时出错。"</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>然后是准备 agent 工具，这里以 http_get 为例：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token triple-quoted-string string">"""HTTP GET 工具：通过 requests 执行简单的 GET 请求并返回预览内容。"""</span>
<span class="token keyword">try</span><span class="token punctuation">:</span>
    <span class="token keyword">import</span> requests
<span class="token keyword">except</span> Exception<span class="token punctuation">:</span>
    requests <span class="token operator">=</span> <span class="token boolean">None</span>


<span class="token keyword">def</span> <span class="token function">http_get</span><span class="token punctuation">(</span>url<span class="token punctuation">:</span> <span class="token builtin">str</span> <span class="token operator">=</span> <span class="token string">""</span><span class="token punctuation">,</span> timeout<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token number">10</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token builtin">str</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""通过 HTTP GET 获取内容，返回状态与部分内容（避免返回过大内容）。"""</span>
    <span class="token keyword">if</span> <span class="token keyword">not</span> url<span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token string">"错误: 未提供 url"</span>
    <span class="token keyword">if</span> requests <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token string">"错误: requests 未安装"</span>
    <span class="token keyword">try</span><span class="token punctuation">:</span>
        r <span class="token operator">=</span> requests<span class="token punctuation">.</span>get<span class="token punctuation">(</span>url<span class="token punctuation">,</span> timeout<span class="token operator">=</span><span class="token builtin">int</span><span class="token punctuation">(</span>timeout<span class="token punctuation">)</span><span class="token punctuation">)</span>
        text <span class="token operator">=</span> r<span class="token punctuation">.</span>text
        <span class="token keyword">return</span> <span class="token string-interpolation"><span class="token string">f"HTTP </span><span class="token interpolation"><span class="token punctuation">&#123;</span>r<span class="token punctuation">.</span>status_code<span class="token punctuation">&#125;</span></span><span class="token string"> </span><span class="token interpolation"><span class="token punctuation">&#123;</span>r<span class="token punctuation">.</span>reason<span class="token punctuation">&#125;</span></span><span class="token string">: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>text<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token format-spec">4000]</span><span class="token punctuation">&#125;</span></span><span class="token string">"</span></span>
    <span class="token keyword">except</span> Exception <span class="token keyword">as</span> e<span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token string-interpolation"><span class="token string">f"错误: http_get 失败: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>e<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>


<p>接下来是 PROMPT，关于 CTF 相关的系统提示词，可以在这上面找到： <a target="_blank" rel="noopener" href="https://docsbot.ai/prompts/tags?tag=CTF">https://docsbot.ai/prompts/tags?tag=CTF</a></p>
<p>尝试构建一个用于 CTF 的 PROMPT，给出可用工具与格式化输出</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">AGENT_SYSTEM_PROMPT <span class="token operator">=</span> <span class="token triple-quoted-string string">"""
你是一个CTF助手，正在尝试完成一道CTF赛题。你的任务是分析用户的请求，并使用可用工具一步步地解决问题，必要时可以自行编写python脚本，使用uv run运行。

# 可用工具:
- `http_get(url="...", timeout=10)`: 通过 HTTP GET 获取指定 URL 的内容，返回状态码和内容。
  * 必要时可以访问页面中的 javascript 文件并分析其中可能出现的路由信息，如出现 `path: "/"` 等字符串。
  * 不要遗漏任何可能包含有用信息的文件，如 robots.txt、sitemap.xml 等。
- `grep(pattern="...", text="...")`: 在给定文本中查找正则表达式 pattern，返回第一个匹配或说明未找到。
- `dirsearch(target="...")`: 对指定 URL 进行目录扫描，返回扫描结果中响应码为 200 的内容。
- `nmap_scan(target="...", options="...")`: 使用 nmap 扫描指定目标，返回扫描结果的结构化数据。
- `run_script(code="...", filename="...")`: 写入并运行指定的 Python 脚本文件，返回其输出结果。
  * 注意：code 参数必须是**单行字符串**。
  * 必须使用 `\\n` 来表示换行。
  * 必须转义内部的双引号（例如 `\\"`），或者在内部使用单引号。

# 工具调用示例:
正确: run_script(code="import os\\nprint('hello')", filename="test.py")
错误: run_script(code="import os
print("hello")", filename="test.py")  &lt;-- 严禁直接换行或未转义引号
  
# 行动格式:
你的回答必须严格遵循以下格式。首先是你的思考过程，然后是你要执行的具体行动，每次回复只输出一对Thought-Action：
Thought: [这里是你的思考过程和下一步计划]
Action: 你决定采取的行动，必须是以下格式之一:
- `function_name(arg_name="arg_value")`:调用一个可用工具。
- `Finish[最终答案]`:当你认为已经获得最终答案时。
- 当你收集到足够的信息，能够回答用户的最终问题时，你必须在Action:字段后使用 Finish[最终答案] 来输出最终答案。

请开始吧！
"""</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>最后组装起来：</p>
<p>main.py</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> OpenAICompatibleClient
<span class="token keyword">import</span> re
<span class="token keyword">from</span> prompt <span class="token keyword">import</span> AGENT_SYSTEM_PROMPT
<span class="token keyword">from</span> tools <span class="token keyword">import</span> http_get<span class="token punctuation">,</span> grep<span class="token punctuation">,</span> dirsearch<span class="token punctuation">,</span> nmap_scan<span class="token punctuation">,</span> run_python_script

available_tools <span class="token operator">=</span> <span class="token punctuation">&#123;</span>
    <span class="token string">"http_get"</span><span class="token punctuation">:</span> <span class="token keyword">lambda</span> <span class="token operator">**</span>kwargs<span class="token punctuation">:</span> http_get<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token string">"grep"</span><span class="token punctuation">:</span> <span class="token keyword">lambda</span> <span class="token operator">**</span>kwargs<span class="token punctuation">:</span> grep<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token string">"dirsearch"</span><span class="token punctuation">:</span> <span class="token keyword">lambda</span> <span class="token operator">**</span>kwargs<span class="token punctuation">:</span> dirsearch<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token string">"nmap_scan"</span><span class="token punctuation">:</span> <span class="token keyword">lambda</span> <span class="token operator">**</span>kwargs<span class="token punctuation">:</span> nmap_scan<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token string">"run_script"</span><span class="token punctuation">:</span> <span class="token keyword">lambda</span> <span class="token operator">**</span>kwargs<span class="token punctuation">:</span> run_python_script<span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token punctuation">&#125;</span>
<span class="token comment"># --- 1. 配置LLM客户端 ---</span>
<span class="token comment"># 请根据您使用的服务，将这里替换成对应的凭证和地址</span>
API_KEY <span class="token operator">=</span> <span class="token string">"lm-studio"</span>
BASE_URL <span class="token operator">=</span> <span class="token string">"http://localhost:1234/v1"</span>
MODEL_ID <span class="token operator">=</span> <span class="token string">"qwen/qwen3-30b-a3b-2507"</span>

llm <span class="token operator">=</span> OpenAICompatibleClient<span class="token punctuation">.</span>OpenAICompatibleClient<span class="token punctuation">(</span>model<span class="token operator">=</span>MODEL_ID<span class="token punctuation">,</span>
                                                    api_key<span class="token operator">=</span>API_KEY<span class="token punctuation">,</span>
                                                    base_url<span class="token operator">=</span>BASE_URL<span class="token punctuation">)</span>

user_prompt <span class="token operator">=</span> <span class="token string">"访问 http://xxx.xxx.xxx.xxx，进行信息收集以获取flag。"</span>
prompt_history <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string-interpolation"><span class="token string">f"用户请求: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>user_prompt<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">]</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"用户输入: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>user_prompt<span class="token punctuation">&#125;</span></span><span class="token string">\n"</span></span> <span class="token operator">+</span> <span class="token string">"="</span> <span class="token operator">*</span> <span class="token number">40</span><span class="token punctuation">)</span>

<span class="token comment"># --- 3. 运行主循环 ---</span>
<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># 设置最大循环次数</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"--- 循环 </span><span class="token interpolation"><span class="token punctuation">&#123;</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">&#125;</span></span><span class="token string"> ---\n"</span></span><span class="token punctuation">)</span>

    <span class="token comment"># 3.1. 构建Prompt</span>
    full_prompt <span class="token operator">=</span> <span class="token string">"\n"</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>prompt_history<span class="token punctuation">)</span>

    <span class="token comment"># 3.2. 调用LLM进行思考</span>
    llm_output <span class="token operator">=</span> llm<span class="token punctuation">.</span>generate<span class="token punctuation">(</span>full_prompt<span class="token punctuation">,</span> system_prompt<span class="token operator">=</span>AGENT_SYSTEM_PROMPT<span class="token punctuation">)</span>
    <span class="token comment"># 模型可能会输出多余的Thought-Action，需要截断</span>
    <span class="token keyword">match</span> <span class="token operator">=</span> re<span class="token punctuation">.</span>search<span class="token punctuation">(</span>
        <span class="token string">r'(Thought:.*?Action:.*?)(?=\n\s*(?:Thought:|Action:|Observation:)|\Z)'</span><span class="token punctuation">,</span>
        llm_output<span class="token punctuation">,</span> re<span class="token punctuation">.</span>DOTALL<span class="token punctuation">)</span>
    <span class="token keyword">if</span> <span class="token keyword">match</span><span class="token punctuation">:</span>
        truncated <span class="token operator">=</span> <span class="token keyword">match</span><span class="token punctuation">.</span>group<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> truncated <span class="token operator">!=</span> llm_output<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            llm_output <span class="token operator">=</span> truncated
            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"已截断多余的 Thought-Action 对"</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"模型输出:\n</span><span class="token interpolation"><span class="token punctuation">&#123;</span>llm_output<span class="token punctuation">&#125;</span></span><span class="token string">\n"</span></span><span class="token punctuation">)</span>
    prompt_history<span class="token punctuation">.</span>append<span class="token punctuation">(</span>llm_output<span class="token punctuation">)</span>

    <span class="token comment"># 3.3. 解析并执行行动</span>
    action_match <span class="token operator">=</span> re<span class="token punctuation">.</span>search<span class="token punctuation">(</span><span class="token string">r"Action: (.*)"</span><span class="token punctuation">,</span> llm_output<span class="token punctuation">,</span> re<span class="token punctuation">.</span>DOTALL<span class="token punctuation">)</span>
    <span class="token keyword">if</span> <span class="token keyword">not</span> action_match<span class="token punctuation">:</span>
        observation <span class="token operator">=</span> <span class="token string">"错误: 未能解析到 Action 字段。请确保你的回复严格遵循 'Thought: ... Action: ...' 的格式。"</span>
        observation_str <span class="token operator">=</span> <span class="token string-interpolation"><span class="token string">f"Observation: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>observation<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">&#123;</span>observation_str<span class="token punctuation">&#125;</span></span><span class="token string">\n"</span></span> <span class="token operator">+</span> <span class="token string">"="</span> <span class="token operator">*</span> <span class="token number">40</span><span class="token punctuation">)</span>
        prompt_history<span class="token punctuation">.</span>append<span class="token punctuation">(</span>observation_str<span class="token punctuation">)</span>
        <span class="token keyword">continue</span>
    action_str <span class="token operator">=</span> action_match<span class="token punctuation">.</span>group<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">if</span> action_str<span class="token punctuation">.</span>startswith<span class="token punctuation">(</span><span class="token string">"Finish"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        final_answer <span class="token operator">=</span> re<span class="token punctuation">.</span><span class="token keyword">match</span><span class="token punctuation">(</span><span class="token string">r"Finish\[(.*)\]"</span><span class="token punctuation">,</span> action_str<span class="token punctuation">)</span><span class="token punctuation">.</span>group<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"任务完成，最终答案: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>final_answer<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
        <span class="token keyword">break</span>

    tool_name <span class="token operator">=</span> re<span class="token punctuation">.</span>search<span class="token punctuation">(</span><span class="token string">r"(\w+)\("</span><span class="token punctuation">,</span> action_str<span class="token punctuation">)</span><span class="token punctuation">.</span>group<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
    args_str <span class="token operator">=</span> re<span class="token punctuation">.</span>search<span class="token punctuation">(</span><span class="token string">r"\((.*)\)"</span><span class="token punctuation">,</span> action_str<span class="token punctuation">)</span><span class="token punctuation">.</span>group<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
    kwargs <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span>re<span class="token punctuation">.</span>findall<span class="token punctuation">(</span><span class="token string">r'(\w+)="([^"]*)"'</span><span class="token punctuation">,</span> args_str<span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">if</span> tool_name <span class="token keyword">in</span> available_tools<span class="token punctuation">:</span>
        observation <span class="token operator">=</span> available_tools<span class="token punctuation">[</span>tool_name<span class="token punctuation">]</span><span class="token punctuation">(</span><span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        observation <span class="token operator">=</span> <span class="token string-interpolation"><span class="token string">f"错误:未定义的工具 '</span><span class="token interpolation"><span class="token punctuation">&#123;</span>tool_name<span class="token punctuation">&#125;</span></span><span class="token string">'"</span></span>

    <span class="token comment"># 3.4. 记录观察结果</span>
    observation_str <span class="token operator">=</span> <span class="token string-interpolation"><span class="token string">f"Observation: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>observation<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">&#123;</span>observation_str<span class="token punctuation">&#125;</span></span><span class="token string">\n"</span></span> <span class="token operator">+</span> <span class="token string">"="</span> <span class="token operator">*</span> <span class="token number">40</span><span class="token punctuation">)</span>
    prompt_history<span class="token punctuation">.</span>append<span class="token punctuation">(</span>observation_str<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>运行，这里展示部分输出</p>
<pre class="line-numbers language-txt" data-language="txt"><code class="language-txt">用户输入: 访问 http://xxx.xxx.xxx.xxx，进行信息收集以获取flag。
========================================
--- 循环 1 ---

正在调用大语言模型...
大语言模型响应成功。
模型输出:
Thought: 我需要先访问目标网站，获取其基础信息。首先尝试使用 http_get 获取主页内容，查看是否有明显线索或隐藏的 flag。
Action: http_get(url="http://xxx.xxx.xxx.xxx", timeout=10)

Observation: HTTP 200 OK: Hello,你知道信息泄露吗?
========================================
--- 循环 2 ---

正在调用大语言模型...
大语言模型响应成功。
已截断多余的 Thought-Action 对
模型输出:
Thought: 主页返回了提示“Hello,你知道信息泄露吗?”，这暗示可能存在信息泄露的路径或隐藏内容。我需要进一步扫描可能的敏感路径，例如robots.txt、sitemap.xml或常见的隐藏文件。
Action: dirsearch(target="http://xxx.xxx.xxx.xxx")

Observation: EXIT 0: [11:49:19] 403 -  299B  - http://xxx.xxx.xxx.xxx/.ht_wsr.txt
[11:49:19] 403 -  302B  - http://xxx.xxx.xxx.xxx/.htaccess.save
[11:49:19] 403 -  304B  - http://xxx.xxx.xxx.xxx/.htaccess.sample
[11:49:19] 403 -  302B  - http://xxx.xxx.xxx.xxx/.htaccess.orig
[11:49:19] 403 -  302B  - http://xxx.xxx.xxx.xxx/.htaccess.bak1
[11:49:19] 403 -  300B  - http://xxx.xxx.xxx.xxx/.htaccess_sc
[11:49:19] 403 -  303B  - http://xxx.xxx.xxx.xxx/.htaccess_extra
[11:49:19] 403 -  302B  - http://xxx.xxx.xxx.xxx/.htaccess_orig
[11:49:19] 403 -  300B  - http://xxx.xxx.xxx.xxx/.htaccessBAK
[11:49:19] 403 -  300B  - http://xxx.xxx.xxx.xxx/.htaccessOLD
[11:49:19] 403 -  301B  - http://xxx.xxx.xxx.xxx/.htaccessOLD2
[11:49:19] 403 -  293B  - http://xxx.xxx.xxx.xxx/.html
[11:49:19] 403 -  292B  - http://xxx.xxx.xxx.xxx/.htm
[11:49:19] 403 -  298B  - http://xxx.xxx.xxx.xxx/.htpasswds
[11:49:19] 403 -  299B  - http://xxx.xxx.xxx.xxx/.httr-oauth
[11:49:19] 403 -  302B  - http://xxx.xxx.xxx.xxx/.htpasswd_test
[11:49:45] 403 -  301B  - http://xxx.xxx.xxx.xxx/server-status
[11:49:45] 403 -  302B  - http://xxx.xxx.xxx.xxx/server-status/
[11:49:52] 200 -  202B  - http://xxx.xxx.xxx.xxx/www.zip

========================================
--- 循环 3 ---

正在调用大语言模型...
大语言模型响应成功。
模型输出:
Thought: dirsearch 扫描发现了一个名为 `www.zip` 的文件，响应码为 200，这很可能是一个压缩包，可能包含敏感信息或 flag。我需要下载该文件并解压以检查其内容。
Action: http_get(url="http://xxx.xxx.xxx.xxx/www.zip", timeout=10)

Observation: HTTP 200 OK: P��T�&lt;
VV  index.php&lt;?php
echo "Hello,你知道信息泄露吗?";
$flag = "flag&#123;you_find_me&#125;";
?>P��T�&lt;
VV   index.phpPK7&#125;
========================================
--- 循环 4 ---

正在调用大语言模型...
大语言模型响应成功。
模型输出:
Thought: 下载的 `www.zip` 文件中包含一个名为 `index.php` 的文件，其内容中直接包含了 flag：`flag&#123;you_find_me&#125;`。这表明 flag 已经明文存在于压缩包的源代码中。  
Action: Finish[flag&#123;you_find_me&#125;]

任务完成，最终答案: flag&#123;you_find_me&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>可以看到这个案例演示了基于<code>Thought-Action-Observation</code>范式的智能体所具备的四项基本能力：任务分解、工具调用、上下文理解和结果合成</p>
<hr>
<h2 id="协作模式"><a href="#协作模式" class="headerlink" title="协作模式"></a>协作模式</h2><h3 id="作为开发者工具"><a href="#作为开发者工具" class="headerlink" title="作为开发者工具"></a>作为开发者工具</h3><p>GitHubCopilot、Claude Code、Trae、Cursor</p>
<h3 id="作为自主协作者"><a href="#作为自主协作者" class="headerlink" title="作为自主协作者"></a>作为自主协作者</h3><p>架构范式可以归纳为几个主流方向：</p>
<ul>
<li>单智能体自主循环：以 <strong>AgentGPT</strong> 为代表，其核心是一个通用智能体通过“思考-规划-执行-反思”的闭环，不断进行自我提示和迭代，以完成一个开放式的高层级目标</li>
<li>多智能体协作：这是当前最主流的探索方向，旨在通过模拟人类团队的协作模式来解决复杂问题。它又可细分为不同模式： <strong>角色扮演式对话</strong>：如 <strong>CAMEL</strong> 框架，通过为两个智能体（例如，“程序员”和“产品经理”）设定明确的角色和沟通协议，让它们在一个结构化的对话中协同完成任务。 <strong>组织化工作流</strong>：如 <strong>MetaGPT</strong> 和 <strong>CrewAI</strong>，它们模拟一个分工明确的“虚拟团队”（如软件公司或咨询小组）。每个智能体都有预设的职责和工作流程（SOP），通过层级化或顺序化的方式协作，产出高质量的复杂成果（如完整的代码库或研究报告）。<strong>AutoGen</strong> 和 <strong>AgentScope</strong> 则提供了更灵活的对话模式，允许开发者自定义智能体间的复杂交互网络。</li>
<li>高级控制流架构：诸如 <strong>LangGraph</strong> 等框架，则更侧重于为智能体提供更强大的底层工程基础。它将智能体的执行过程建模为状态图（State Graph），从而能更灵活、更可靠地实现循环、分支、回溯以及人工介入等复杂流程。</li>
</ul>
<p>对于 CTF 多智能体协作的应用，可以参考一下这个： <a target="_blank" rel="noopener" href="https://github.com/yhy0/CHYing-agent">https://github.com/yhy0/CHYing-agent</a></p>
<h3 id="Workflow-与-Agent-的差异"><a href="#Workflow-与-Agent-的差异" class="headerlink" title="Workflow 与 Agent 的差异"></a>Workflow 与 Agent 的差异</h3><p>Workflow 是让 AI 按部就班地执行指令，而 Agent 则是赋予 AI 自由度去自主达成目标。</p>
<hr>
<h2 id="正式构建智能体经典范式"><a href="#正式构建智能体经典范式" class="headerlink" title="正式构建智能体经典范式"></a>正式构建智能体经典范式</h2><ul>
<li><strong>ReAct (Reasoning and Acting)：</strong> 一种将“思考”和“行动”紧密结合的范式，让智能体边想边做，动态调整。</li>
<li><strong>Plan-and-Solve：</strong> 一种“三思而后行”的范式，智能体首先生成一个完整的行动计划，然后严格执行。</li>
<li><strong>Reflection：</strong> 一种赋予智能体“反思”能力的范式，通过自我批判和修正来优化结果。</li>
</ul>
<p>很明显对于 CTF 而言，ReAct 是最正确的选择</p>
<h3 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h3><p>uv，python 3.12</p>
<p>openai 库：与大语言模型交互</p>
<p>python-dotenv 库：安全管理 API 密钥</p>
<h3 id="配置客户端"><a href="#配置客户端" class="headerlink" title="配置客户端"></a>配置客户端</h3><p>配置 API：</p>
<p>.env 文件</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">LLM_API_KEY<span class="token operator">=</span><span class="token string">"lm-studio"</span>
LLM_MODEL_ID<span class="token operator">=</span><span class="token string">"qwen/qwen3-30b-a3b-2507"</span>
LLM_BASE_URL<span class="token operator">=</span><span class="token string">"http://localhost:1234/v1"</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>


<h3 id="封装基础-LLM-调用函数"><a href="#封装基础-LLM-调用函数" class="headerlink" title="封装基础 LLM 调用函数"></a>封装基础 LLM 调用函数</h3><p>CTFAgentsLLM.py</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> os
<span class="token keyword">from</span> dotenv <span class="token keyword">import</span> load_dotenv
<span class="token keyword">from</span> openai <span class="token keyword">import</span> OpenAI
<span class="token keyword">from</span> typing <span class="token keyword">import</span> List<span class="token punctuation">,</span> Dict

load_dotenv<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">import</span> os
<span class="token keyword">from</span> openai <span class="token keyword">import</span> OpenAI
<span class="token keyword">from</span> dotenv <span class="token keyword">import</span> load_dotenv
<span class="token keyword">from</span> typing <span class="token keyword">import</span> List<span class="token punctuation">,</span> Dict

<span class="token comment"># 加载 .env 文件中的环境变量</span>
load_dotenv<span class="token punctuation">(</span><span class="token punctuation">)</span>


<span class="token keyword">class</span> <span class="token class-name">CTFAgentsLLM</span><span class="token punctuation">:</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>
                 model<span class="token punctuation">:</span> <span class="token builtin">str</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
                 apiKey<span class="token punctuation">:</span> <span class="token builtin">str</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
                 baseUrl<span class="token punctuation">:</span> <span class="token builtin">str</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
                 timeout<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        初始化客户端。优先使用传入参数，如果未提供，则从环境变量加载。
        """</span>
        self<span class="token punctuation">.</span>model <span class="token operator">=</span> model <span class="token keyword">or</span> os<span class="token punctuation">.</span>getenv<span class="token punctuation">(</span><span class="token string">"LLM_MODEL_ID"</span><span class="token punctuation">)</span>
        apiKey <span class="token operator">=</span> apiKey <span class="token keyword">or</span> os<span class="token punctuation">.</span>getenv<span class="token punctuation">(</span><span class="token string">"LLM_API_KEY"</span><span class="token punctuation">)</span>
        baseUrl <span class="token operator">=</span> baseUrl <span class="token keyword">or</span> os<span class="token punctuation">.</span>getenv<span class="token punctuation">(</span><span class="token string">"LLM_BASE_URL"</span><span class="token punctuation">)</span>
        timeout <span class="token operator">=</span> timeout <span class="token keyword">or</span> <span class="token builtin">int</span><span class="token punctuation">(</span>os<span class="token punctuation">.</span>getenv<span class="token punctuation">(</span><span class="token string">"LLM_TIMEOUT"</span><span class="token punctuation">,</span> <span class="token number">60</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

        <span class="token keyword">if</span> <span class="token keyword">not</span> <span class="token builtin">all</span><span class="token punctuation">(</span><span class="token punctuation">[</span>self<span class="token punctuation">.</span>model<span class="token punctuation">,</span> apiKey<span class="token punctuation">,</span> baseUrl<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"模型ID、API密钥和服务地址必须被提供或在.env文件中定义。"</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>client <span class="token operator">=</span> OpenAI<span class="token punctuation">(</span>api_key<span class="token operator">=</span>apiKey<span class="token punctuation">,</span> base_url<span class="token operator">=</span>baseUrl<span class="token punctuation">,</span> timeout<span class="token operator">=</span>timeout<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">think</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>
              messages<span class="token punctuation">:</span> List<span class="token punctuation">[</span>Dict<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">,</span> <span class="token builtin">str</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
              temperature<span class="token punctuation">:</span> <span class="token builtin">float</span> <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> <span class="token builtin">str</span><span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"🧠 正在调用 </span><span class="token interpolation"><span class="token punctuation">&#123;</span>self<span class="token punctuation">.</span>model<span class="token punctuation">&#125;</span></span><span class="token string"> 模型..."</span></span><span class="token punctuation">)</span>
        <span class="token keyword">try</span><span class="token punctuation">:</span>
            response <span class="token operator">=</span> self<span class="token punctuation">.</span>client<span class="token punctuation">.</span>chat<span class="token punctuation">.</span>completions<span class="token punctuation">.</span>create<span class="token punctuation">(</span>
                model<span class="token operator">=</span>self<span class="token punctuation">.</span>model<span class="token punctuation">,</span>
                messages<span class="token operator">=</span>messages<span class="token punctuation">,</span>
                temperature<span class="token operator">=</span>temperature<span class="token punctuation">,</span>
                stream<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
            <span class="token punctuation">)</span>

            <span class="token comment"># 处理流式响应</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"✅ 大语言模型响应成功:"</span><span class="token punctuation">)</span>
            collected_content <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
            <span class="token keyword">for</span> chunk <span class="token keyword">in</span> response<span class="token punctuation">:</span>
                content <span class="token operator">=</span> chunk<span class="token punctuation">.</span>choices<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>delta<span class="token punctuation">.</span>content <span class="token keyword">or</span> <span class="token string">""</span>
                <span class="token keyword">print</span><span class="token punctuation">(</span>content<span class="token punctuation">,</span> end<span class="token operator">=</span><span class="token string">""</span><span class="token punctuation">,</span> flush<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
                collected_content<span class="token punctuation">.</span>append<span class="token punctuation">(</span>content<span class="token punctuation">)</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 在流式输出结束后换行</span>
            <span class="token keyword">return</span> <span class="token string">""</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>collected_content<span class="token punctuation">)</span>

        <span class="token keyword">except</span> Exception <span class="token keyword">as</span> e<span class="token punctuation">:</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"❌ 调用LLM API时发生错误: </span><span class="token interpolation"><span class="token punctuation">&#123;</span>e<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
            <span class="token keyword">return</span> <span class="token boolean">None</span>


<span class="token comment"># --- 客户端使用示例 ---</span>
<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    <span class="token keyword">try</span><span class="token punctuation">:</span>
        llmClient <span class="token operator">=</span> CTFAgentsLLM<span class="token punctuation">(</span><span class="token punctuation">)</span>

        exampleMessages <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">&#123;</span>
            <span class="token string">"role"</span><span class="token punctuation">:</span>
            <span class="token string">"system"</span><span class="token punctuation">,</span>
            <span class="token string">"content"</span><span class="token punctuation">:</span>
            <span class="token string">"You are a helpful assistant that writes Python code."</span>
        <span class="token punctuation">&#125;</span><span class="token punctuation">,</span> <span class="token punctuation">&#123;</span>
            <span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"user"</span><span class="token punctuation">,</span>
            <span class="token string">"content"</span><span class="token punctuation">:</span> <span class="token string">"写一个DES CBC加密的Python代码示例。"</span>
        <span class="token punctuation">&#125;</span><span class="token punctuation">]</span>

        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"--- 调用LLM ---"</span><span class="token punctuation">)</span>
        responseText <span class="token operator">=</span> llmClient<span class="token punctuation">.</span>think<span class="token punctuation">(</span>exampleMessages<span class="token punctuation">)</span>
        <span class="token keyword">if</span> responseText<span class="token punctuation">:</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"\n\n--- 完整模型响应 ---"</span><span class="token punctuation">)</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span>responseText<span class="token punctuation">)</span>

    <span class="token keyword">except</span> ValueError <span class="token keyword">as</span> e<span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>e<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>


<hr>
<h3 id="ReAct-范式"><a href="#ReAct-范式" class="headerlink" title="ReAct 范式"></a>ReAct 范式</h3><p>思考与行动相辅相成</p>
<p>在我们实现的第一个智能体中即使用了 ReAct 范式：</p>
<ul>
<li><strong>Thought (思考)：</strong> 这是智能体的“内心独白”。它会分析当前情况、分解任务、制定下一步计划，或者反思上一步的结果。</li>
<li><strong>Action (行动)：</strong> 这是智能体决定采取的具体动作，通常是调用一个外部工具，例如 <code>Search[&#39;华为最新款手机&#39;]</code>。</li>
<li><strong>Observation (观察)：</strong> 这是执行 <code>Action</code> 后从外部工具返回的结果，例如搜索结果的摘要或API的返回值。</li>
</ul>
<p>智能体将不断重复这个 <strong>Thought -&gt; Action -&gt; Observation</strong> 的循环，将新的观察结果追加到历史记录中，形成一个不断增长的上下文，直到它在 <code>Thought</code> 中认为已经找到了最终答案，然后输出结果。这个过程形成了一个强大的协同效应：<strong>推理使得行动更具目的性，而行动则为推理提供了事实依据。</strong></p>
<p>这种机制特别适用于以下场景：</p>
<ul>
<li><strong>需要外部知识的任务</strong>：如查询实时信息（天气、新闻、股价）、搜索专业领域的知识等。</li>
<li><strong>需要精确计算的任务</strong>：将数学问题交给计算器工具，避免LLM的计算错误。</li>
<li><strong>需要与API交互的任务</strong>：如操作数据库、调用某个服务的API来完成特定功能。</li>
</ul>
<h3 id="工具的定义与实现"><a href="#工具的定义与实现" class="headerlink" title="工具的定义与实现"></a>工具的定义与实现</h3><p>一个良好定义的工具应包含以下三个核心要素：</p>
<ol>
<li>**名称 (Name)**： 一个简洁、唯一的标识符，供智能体在 <code>Action</code> 中调用，例如 <code>Search</code>。</li>
<li><strong>描述 (Description)<strong>： 一段清晰的自然语言描述，说明这个工具的用途。</strong>这是整个机制中最关键的部分</strong>，因为大语言模型会依赖这段描述来判断何时使用哪个工具。</li>
<li>**执行逻辑 (Execution Logic)**： 真正执行任务的函数或方法</li>
</ol>
<hr>
<h1 id="MCP"><a href="#MCP" class="headerlink" title="MCP"></a>MCP</h1><p>这里使用 vscode，版本 1.105.1，原先的扩展商店处下方有 MCP 扩展，点击即可启用 <code>chat.mcp.gallery.enabled</code> （setting.json 中）</p>
<h2 id="hello-world"><a href="#hello-world" class="headerlink" title="hello world"></a>hello world</h2><p>第一个 mcp 搭建： <a target="_blank" rel="noopener" href="https://www.runoob.com/vscode/vscode-mcp-servers.html">https://www.runoob.com/vscode/vscode-mcp-servers.html</a></p>
<p>准备一个 test.py</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> sys
<span class="token keyword">import</span> json

<span class="token comment"># 读取 MCP 初始化请求</span>
_ <span class="token operator">=</span> json<span class="token punctuation">.</span>load<span class="token punctuation">(</span>sys<span class="token punctuation">.</span>stdin<span class="token punctuation">)</span>

<span class="token comment"># 输出 MCP 响应（标准 JSON）</span>
json<span class="token punctuation">.</span>dump<span class="token punctuation">(</span><span class="token punctuation">&#123;</span>
    <span class="token string">"type"</span><span class="token punctuation">:</span> <span class="token string">"text"</span><span class="token punctuation">,</span>
    <span class="token string">"text"</span><span class="token punctuation">:</span> <span class="token string">"Hello World from MCP!"</span>
<span class="token punctuation">&#125;</span><span class="token punctuation">,</span> sys<span class="token punctuation">.</span>stdout<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>mcp.json</p>
<pre class="line-numbers language-json" data-language="json"><code class="language-json"><span class="token punctuation">&#123;</span>
  <span class="token property">"servers"</span><span class="token operator">:</span> <span class="token punctuation">&#123;</span>
    <span class="token property">"HelloWorldServer"</span><span class="token operator">:</span> <span class="token punctuation">&#123;</span>
      <span class="token property">"type"</span><span class="token operator">:</span> <span class="token string">"stdio"</span><span class="token punctuation">,</span>
      <span class="token property">"command"</span><span class="token operator">:</span> <span class="token string">"python3"</span><span class="token punctuation">,</span>
      <span class="token property">"args"</span><span class="token operator">:</span> <span class="token punctuation">[</span><span class="token string">"test.py"</span><span class="token punctuation">]</span>
    <span class="token punctuation">&#125;</span>
  <span class="token punctuation">&#125;</span>
<span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>


<hr>
<h2 id="IDA-PRO-MCP"><a href="#IDA-PRO-MCP" class="headerlink" title="IDA-PRO-MCP"></a>IDA-PRO-MCP</h2><p>参考：</p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/alexander17/p/19089720">https://www.cnblogs.com/alexander17/p/19089720</a></p>
<p><a target="_blank" rel="noopener" href="https://bbs.kanxue.com/thread-286813.htm">https://bbs.kanxue.com/thread-286813.htm</a></p>
<p>首先先确定自己 ida 用的 python 版本，需要 &gt;&#x3D; 3.11.0</p>
<p>具体在 ida 安装目录下使用 idapyswitch 查看</p>
<p><img src="/blog/2024/05/05/AI%E5%85%A5%E9%97%A8%E5%88%B0%E5%85%A5%E5%9C%9F/IMG-20251124163915692.png"></p>
<p>然后使用对应版本的 python 安装 ida-pro-mcp，这里使用 pipx 方便版本管理</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">pipx <span class="token function">install</span> <span class="token parameter variable">--python</span> python3.13 git+https://github.com/mrexodia/ida-pro-mcp<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<p>pipx 安装完 ida-pro-mcp 依赖后执行该依赖</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">ida-pro-mcp <span class="token parameter variable">--install</span>
ida-pro-mcp <span class="token parameter variable">--config</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<p>前者会将插件安装到 IDA PRO 中</p>
<p><img src="/blog/2024/05/05/AI%E5%85%A5%E9%97%A8%E5%88%B0%E5%85%A5%E5%9C%9F/IMG-20251124164235267.png"></p>
<p>后者生成对应的 MCP 连接格式</p>
<pre class="line-numbers language-json" data-language="json"><code class="language-json"><span class="token punctuation">[</span>HTTP MCP CONFIGURATION<span class="token punctuation">]</span>
<span class="token punctuation">&#123;</span>
  <span class="token property">"mcpServers"</span><span class="token operator">:</span> <span class="token punctuation">&#123;</span>
    <span class="token property">"ida-pro-mcp"</span><span class="token operator">:</span> <span class="token punctuation">&#123;</span>
      <span class="token property">"type"</span><span class="token operator">:</span> <span class="token string">"http"</span><span class="token punctuation">,</span>
      <span class="token property">"url"</span><span class="token operator">:</span> <span class="token string">"http://127.0.0.1:13337/mcp"</span>
    <span class="token punctuation">&#125;</span>
  <span class="token punctuation">&#125;</span>
<span class="token punctuation">&#125;</span>

<span class="token punctuation">[</span>STDIO MCP CONFIGURATION<span class="token punctuation">]</span>
<span class="token punctuation">&#123;</span>
  <span class="token property">"mcpServers"</span><span class="token operator">:</span> <span class="token punctuation">&#123;</span>
    <span class="token property">"ida-pro-mcp"</span><span class="token operator">:</span> <span class="token punctuation">&#123;</span>
      <span class="token property">"command"</span><span class="token operator">:</span> <span class="token string">"/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/bin/python3"</span><span class="token punctuation">,</span>
      <span class="token property">"args"</span><span class="token operator">:</span> <span class="token punctuation">[</span>
        <span class="token string">"/Users/username/.local/pipx/venvs/ida-pro-mcp/lib/python3.13/site-packages/ida_pro_mcp/server.py"</span><span class="token punctuation">,</span>
        <span class="token string">"--ida-rpc"</span><span class="token punctuation">,</span>
        <span class="token string">"http://127.0.0.1:13337"</span>
      <span class="token punctuation">]</span>
    <span class="token punctuation">&#125;</span>
  <span class="token punctuation">&#125;</span>
<span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>然后这里以 copliot 为例，导入 MCP 并调用</p>
<p>首先启动 IDA 中的 MCP plugin 服务</p>
<p><img src="/blog/2024/05/05/AI%E5%85%A5%E9%97%A8%E5%88%B0%E5%85%A5%E5%9C%9F/IMG-20251124164540450.png"></p>
<p><img src="/blog/2024/05/05/AI%E5%85%A5%E9%97%A8%E5%88%B0%E5%85%A5%E5%9C%9F/IMG-20251124164621179.png"></p>
<p>然后配置工具导入，直接以 http 方式连接，输入上面的 HTTP 链接即可</p>
<p><img src="/blog/2024/05/05/AI%E5%85%A5%E9%97%A8%E5%88%B0%E5%85%A5%E5%9C%9F/IMG-20251124164728595.png"></p>
<p><img src="/blog/2024/05/05/AI%E5%85%A5%E9%97%A8%E5%88%B0%E5%85%A5%E5%9C%9F/IMG-20251124164844968.png"></p>
<p>然后大模型就能够调用 IDA 进行分析了</p>

    </div>
    
    
    
    
    
    
    
</div>

      </div>
    </transition>
    
    <transition name="fade">
      <div id="preview" ref="preview" v-show="previewShow">
        <img id="preview-content" ref="previewContent" />
      </div>
    </transition>
    
  </div>
  <script src="/blog/js/main.js"></script>
  
  




  
  

<div id="vcomments"></div>
<script defer src="/blog/js/lib/valine.min.js" onload="
    new Valine({
        el: '#vcomments',
        app_id: 'U9mZLKunkRUTcBFthjXPjnZE-9Nh9j0Va',
        app_key: 'SzWOnY0PWLGNuL6jW8TZYc7j',
        visitor: 'true',
        placeholder: '写点什么吧(●ˇ∀ˇ●)',
        serverURLs: 'https://u9mzlkun.lc-cn-n1-shared.com',
    });
"></script>


  <footer id="footer">
    <div id="footer-wrap">
        <div>
            &copy;
            2022 - 2026 雲流のLowest World
            <span id="footer-icon">
                <i class="fa-solid fa-font-awesome fa-fw"></i>
            </span>
            &commat;C1oudfL0w0
        </div>
        <div>
            Based on the <a target="_blank" rel="noopener" href="https://hexo.io">Hexo Engine</a> &amp;
            <a target="_blank" rel="noopener" href="https://github.com/argvchs/hexo-theme-particlex">ParticleX Theme</a>
        </div>
        
    </div>
</footer>

<script src="/blog/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/blog/live2dw/assets/z16.model.json"},"display":{"position":"left","width":150,"height":290},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>

</html>